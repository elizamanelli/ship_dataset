{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading your own ship dataset from Planet.com\n",
    "\n",
    "The code provided here serves as an example of how to download satellite imagery from planet.com to form a dataset. The dataset created here is of images either including ships or not and can be used to train a ship recognition model.\n",
    "\n",
    "The thought process is also explained in my blog post here: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import shapely.wkt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The datasources\n",
    "\n",
    "Our satellite imagery comes from planet.com. We're using the PlanetScope 4Band Images which have RGB+IR channels and a resolution of 3 * 3m. Those are downloaded using the porder API which is explained here (https://medium.com/@samapriyaroy/order-up-using-and-building-with-planet-s-new-ordersv2-api-ba2fe14eac8e).\n",
    "\n",
    "The second datasource is historical AIS data downloaded from https://marinecadastre.gov/ais/. \n",
    "\n",
    "The third thing we need is the .geojson file. The one here is created using http://geojson.io/#map=2/20.0/0.0 and can be found in the folder /geojson/miami_to_palm.geojson .\n",
    "\n",
    "So first we have to set some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 12\n",
    "year = 2017\n",
    "num_days_in_month = 31\n",
    "geojson = \"miami_to_palm.geojson\"\n",
    "ais_file = \"AIS_2017_12_Zone17.csv\"\n",
    "path_extension = file_extension= ais_file.split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we're creating our dataset month-by-month. This means you have to run this multiple times, but it helps if you are on a quota constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use:\n",
    "The notebook is constructed to be run from top to bottom. The steps do have a certain order - you for example can't download the data before ordering it. And there's an extra layer of caution after ordering - planet takes a while to process your orders. Therefore the code fails if you don't wait long enough before running the downloading cell!\n",
    "\n",
    "**Don't use \"run all\"!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the original AIS file\n",
    "\n",
    "Because the original AIS file is giant, we first reduce it to only include ships that were tracked in the area inside our .geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_ais_csv(filename, geojson_name):\n",
    "    # reduce the ais csv to contain only the region of interest -- takes forever!\n",
    "    print('Reducing the original AIS file - this might take a while...')\n",
    "    data_dir = os.path.join(os.getcwd(), 'data')\n",
    "    ais_org_dir = os.path.join(data_dir, 'original_ais')\n",
    "    csv_path = os.path.join(ais_org_dir, filename)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    def to_coordinate(df):\n",
    "        # We convert the Longitude and Latidute to Koordinates so we can handle it with geopandas\n",
    "        coordinate = Point(df.loc['LON'], df.loc['LAT'])\n",
    "        return coordinate\n",
    "    df['coordinate'] = df.apply(to_coordinate, axis=1, raw=True)\n",
    "    geojson_dir = os.path.join(data_dir, \"geojson\")\n",
    "    path_to_geojson = os.path.join(geojson_dir, geojson_name)\n",
    "    gjson = gpd.read_file(path_to_geojson)\n",
    "    # We select the first shape (our actual geojson shape we placed in the file)\n",
    "    gjson = gjson.iloc[0]\n",
    "    # And convert it to a GeoSeries\n",
    "    series = gpd.GeoSeries(gjson)\n",
    "    # We check that the ship coordinates are within the .geojson\n",
    "    reduced_index = df.loc[df.loc[:,'coordinate'].apply(lambda x: x.within(series[0])),:].index\n",
    "    # And reduce the whole thing\n",
    "    reduced_df = df.loc[reduced_index,:]\n",
    "    # we convert the date to datetime to easier handle it\n",
    "    reduced_df['datetime'] = reduced_df.apply(lambda x: datetime.datetime.fromisoformat(x.loc['BaseDateTime']), axis=1)\n",
    "    # and save the file\n",
    "    ais_reduced_dir = os.path.join(data_dir, 'reduced_ais')\n",
    "    reduced_csv_path = os.path.join(ais_reduced_dir, filename)\n",
    "    reduced_df.to_csv(reduced_csv_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load or construct the reduced df\n",
    "try:\n",
    "    data_dir = os.path.join(os.getcwd(), 'data')\n",
    "    ais_reduced_dir = os.path.join(data_dir, 'reduced_ais')\n",
    "    reduced_csv_path = os.path.join(ais_reduced_dir, ais_file)\n",
    "    reduced_df = pd.read_csv(reduced_csv_path)\n",
    "    print('Dataframe loaded')\n",
    "except FileNotFoundError:\n",
    "    print('Reduced Version not found, creating it ....')\n",
    "    reduce_ais_csv(ais_file, geojson)\n",
    "    reduced_df = pd.read_csv(reduced_csv_path)\n",
    "    print('Dataframe created and loaded')\n",
    "reduced_df['datetime'] = reduced_df.apply(lambda x: datetime.datetime.fromisoformat(x.loc['BaseDateTime']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the satellite times\n",
    "\n",
    "Next we query the timestamps of the satellite images available for that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_id_list(geojson_name, date):\n",
    "    # Creates an id list of all data that is available in the geojson for this date. We need this, so we can \n",
    "    # check where there were boats while the satellite took it's picture.\n",
    "    data_dir = os.path.join(os.getcwd(), 'data')\n",
    "    geo_dir = os.path.join(data_dir, 'geojson')\n",
    "    id_dir = os.path.join(data_dir, 'idlists')\n",
    "    in_file = os.path.join(geo_dir, geojson_name)\n",
    "    out_file = os.path.join(id_dir, str(date) + \"idlist.csv\")\n",
    "    end_date = date + datetime.timedelta(days=1)#datetime.date(date.year, date.month, date.day+1)\n",
    "    # We create a command to query the idlist using the geojson, max 50 % cloud coverage and at least 80% overlap with our geojson\n",
    "    command_1 = \"porder idlist --input \\\"\" + str(in_file) + \"\\\" \"\n",
    "    command_2 = \"--start \\\"\" + str(date) + \"\\\" --end \\\"\" + str(end_date) + \"\\\" --item \\\"PSScene4Band\\\" --asset \\\"analytic\\\"\"\n",
    "    command_3 = \" --cmax 0.5 --outfile \\\"\" + str(out_file) +  \"\\\" --overlap 80\" \n",
    "    command = command_1 + command_2 + command_3\n",
    "    query = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, stderr = subprocess.PIPE)\n",
    "    result = query.stdout.read().decode(\"utf-8\")\n",
    "    # We compute the estimated cost (in km2)\n",
    "    try:\n",
    "        estimated_cost = np.float(result[result.find('clipped:')+9:].split(' ')[0].replace(',',''))\n",
    "    except ValueError:\n",
    "        estimated_cost = 0\n",
    "    return estimated_cost, out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(file_name):\n",
    "    # extract the time from the filename given in the id_list\n",
    "    time_string = file_name.split('_')[1]\n",
    "    date_string = file_name.split('_')[0]\n",
    "    start_time = datetime.datetime(year=np.int(date_string[0:4]), month=np.int(date_string[4:6]),\n",
    "                                   day= np.int(date_string[6:8]), hour=np.int(time_string[0:2]), \n",
    "                                   minute=np.int(time_string[2:4]))\n",
    "    return start_time\n",
    "\n",
    "def get_unique_times(file_name):\n",
    "    # read the id list file and extract the times when the pictures were taken.\n",
    "    file = pd.read_csv(file_name, names=['file_name'])\n",
    "    if file.shape[0] > 0:\n",
    "        file['time'] = file.applymap(get_time)\n",
    "        times = file.loc[:,'time'].unique()\n",
    "    else:\n",
    "        times=[]\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ships(time_list, reduced_df):\n",
    "    # Extract the ships from the reduced ais file using the times gathered from the id list.\n",
    "    entries=pd.DataFrame()\n",
    "    for time in time_list:\n",
    "        # We use a delta of 1 minute since the AIS logging resolution is 1 minute\n",
    "        entries = pd.concat([entries,reduced_df.loc[abs(reduced_df['datetime'] - time) < \n",
    "                                                        datetime.timedelta(minutes=1),:]])\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we check the full month\n",
    "ship_df = pd.DataFrame()\n",
    "# iterating over all days\n",
    "for day in range(1,num_days_in_month+1):\n",
    "    date = datetime.date(year=year, month=month, day=day)\n",
    "    # we query the id_list - so all available filenames\n",
    "    estimated_cost, id_list_file = create_id_list(geojson, date)\n",
    "    print('Imagery for ',  str(date) ,' is around ',  str(estimated_cost), ' sqkm')\n",
    "    # we extract the available times from the id list\n",
    "    times = get_unique_times(id_list_file)\n",
    "    print('Extracted ' , str(len(times)) , ' unique time slots.')\n",
    "    # We intersect this with the AIS data to only select ships that are inside the geojson at the time an image was taken\n",
    "    day_ship_df = get_ships(times, reduced_df)\n",
    "    print('Found ', str(day_ship_df.shape[0]), ' ships.')\n",
    "    ship_df = pd.concat([ship_df, day_ship_df])\n",
    "print(ship_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_path = os.path.join(data_dir, 'ships' + ais_file)\n",
    "ship_df.to_csv(ship_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering the ship files\n",
    "\n",
    "Now we have all our possible ship candidates saved in ship_df. Next we make sure our coordinates are points, so we can access them more readily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_coordinate(df):\n",
    "    # Converts the Lon and LAT information in the dataframe to a geo point\n",
    "    coordinate = Point(df.loc['LON'], df.loc['LAT'])\n",
    "    return coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the coordinates in ship df into points so we can handle them.\n",
    "ship_df['coordinate'] = ship_df.apply(to_coordinate, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set our set the size of our rectangle that we want to draw around the ship that we want to download.\n",
    "This is speciefied in Degrees (on EPSG48 i guess), not pixels or meters, so it's hard to find a correct size. 0.02 workes fine and covers around 1200 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(geometry = ship_df.loc[:,'coordinate'])\n",
    "# buffer 0.02 corresponds to 1200 pixel\n",
    "# buffer 0.01 is to small to download\n",
    "buffer = gdf.buffer(0.02)\n",
    "ship_polygon = buffer.envelope\n",
    "ship_df['envelope'] = ship_polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start ordering the images. We also keep track of the associated cost in km2 since planet has a quota on it for most users.\n",
    "\n",
    "We iterate over all ships and create a geojson corresponding to the rectangle around that one ship, that we have designed before. Next we query planet for this small image, and if it exists we download it. (not all images are available because we have not yet taken into account that the satellite not always covers the full geojson. Sometimes it just covers a small part of it and not all boats are included in this coverage.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_ship(ship, id_list_filename):\n",
    "    # Ordering a ship from planet.com using porder API\n",
    "    data_dir = os.path.join(os.getcwd(), 'data')\n",
    "    try:\n",
    "        name = ship['BaseDateTime'] + str(ship['MMSI'])\n",
    "    except KeyError:\n",
    "        name = str(ship['BaseDateTime']) + str(ship['ship_idx'])\n",
    "    idlist = filename\n",
    "    geojson_dir = os.path.join(data_dir, 'geojson')\n",
    "    geojson = os.path.join(geojson_dir, 'test.geojson')\n",
    "    # We order the cliped image around our boat\n",
    "    command_1 = \"porder order --name \\\"\" + str(name) + \"\\\" \"\n",
    "    command_2 = \"--idlist \\\"\" + str(idlist) + \"\\\" --item \\\"PSScene4Band\\\" --bundle \\\"analytic\\\"\"\n",
    "    command_3 = \" --boundary \\\"\" + str(geojson) + \"\\\" --op clip\" \n",
    "    command = command_1 + command_2 + command_3\n",
    "    #print(command)\n",
    "    query = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, stderr = subprocess.PIPE)\n",
    "    result = query.stdout.read().decode(\"utf-8\")\n",
    "    url = result[result.find('https:') :].split(' ')[0]\n",
    "    # Setting out the order\n",
    "    print(result)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cost = 0\n",
    "downloaded_ships = pd.DataFrame()\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "geojson_dir = os.path.join(data_dir, 'geojson')\n",
    "geojson = os.path.join(geojson_dir, 'test.geojson')\n",
    " \n",
    "for index, ship in ship_df.iterrows():\n",
    "    # We iterate over all possible ships and check if for the small ship region the satellite took a picture e.g. \n",
    "    # there's a quota associated to our requested ship.\n",
    "    # We saveguard our downloaded ships info some times, in case something goes wrong / crashes\n",
    "    if index%100 == 0:\n",
    "        downloaded_ships.to_csv('ship_urls' + str(index) + '.csv')\n",
    "    current_ship = ship.copy()\n",
    "    # we create the geojson\n",
    "    gpd.GeoSeries([ship['envelope']]).to_file(geojson, driver='GeoJSON')\n",
    "    try:\n",
    "        date = datetime.datetime.fromisoformat(ship['datetime']).date()\n",
    "    except TypeError:\n",
    "        date = ship['datetime'].date()\n",
    "    # we query if there's data available for this ship / date\n",
    "    cost, filename = create_id_list(geojson, date)\n",
    "    if cost == 0:\n",
    "        continue\n",
    "    else:\n",
    "        # if so, we make sure it's the correct timing (in case there's multiple satellite images in a day)\n",
    "        times = get_unique_times(filename)\n",
    "        if len(times) > 1:\n",
    "            # this could be done nicer... like using the extra data and not throwing it away\n",
    "            continue\n",
    "        try:\n",
    "            time_dif = datetime.datetime.utcfromtimestamp(times[0].astype('O')/1e9) - datetime.datetime.fromisoformat(ship['datetime'])\n",
    "        except TypeError:\n",
    "            time_dif = datetime.datetime.utcfromtimestamp(times[0].astype('O')/1e9) - ship['datetime']\n",
    "        # We only order if the time is correct\n",
    "        if abs(time_dif) < datetime.timedelta(minutes=1):       \n",
    "            # if we catch a ship we request and download the asset.\n",
    "            # we order the asset\n",
    "            url = order_ship(ship, filename)\n",
    "            current_ship['url'] = url\n",
    "            downloaded_ships = downloaded_ships.append(current_ship)\n",
    "            total_cost +=cost\n",
    "print(total_cost)\n",
    "print(downloaded_ships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "data_set_dir = os.path.join(data_dir, 'ship_downloads')\n",
    "downloaded_ships.to_csv(ship_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't run the next cells right away!**\n",
    "\n",
    "Read first and check if your order has been completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the requested ships\n",
    "\n",
    "Before we download the data we have to wait a while. Our order at planet is taken care of and it sometimes takes a few minutes. You can check if your data is ready by clicking on the downloading URL displayed while ordering. This gives you a current status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ship(url, index, file_path):\n",
    "    #filepath = 'downloaded_ships\\\\AIS_Filename'\n",
    "    data_dir = os.path.join(os.getcwd(), 'data')\n",
    "    url = url\n",
    "    save_path = os.path.join(data_dir, file_path)\n",
    "    idx_save_path = os.path.join(save_path, str(index))\n",
    "    \n",
    "    # We create a directory for the downloaded files\n",
    "    mkdir_command =\"mkdir \" + str(idx_save_path)\n",
    "    subprocess.Popen(mkdir_command, stdout=subprocess.PIPE, shell=True, stderr = subprocess.PIPE)\n",
    "    \n",
    "    # We construct the download command\n",
    "    command_1 = \"porder download --url \\\"\" + str(url) + \"\\\" \"\n",
    "    command_2 = \"--local \\\"\" + str(idx_save_path) + \"\\\"\"\n",
    "    command = command_1 + command_2 \n",
    "\n",
    "    query = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, stderr = subprocess.PIPE)\n",
    "    result = query.stdout.read().decode(\"utf-8\")\n",
    "    print(result)\n",
    "    #extract the correct basefilename\n",
    "    try:\n",
    "        filename = result[result.find('Downloading:') :].split(' ')[1].split('.')[0]\n",
    "    except IndexError:\n",
    "        try:\n",
    "            # print a different error when the file is already downloaded\n",
    "            filename = result[result.find('SKIPPING:') :].split(' ')[1].split('.')[0]\n",
    "        except IndexError:\n",
    "            filename = 'Error'\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'downloaded_ships'\n",
    "base_path = os.path.join(base_path, file_extension)\n",
    "downloaded_ships['base_name'] = np.nan\n",
    "# iterate over all ships\n",
    "for index, row in downloaded_ships.iterrows():\n",
    "    print(index)\n",
    "    url = row.loc['url']\n",
    "    base_name = download_ship(url, index, base_path)\n",
    "    downloaded_ships.loc[index, 'base_name'] = base_name\n",
    "    \n",
    "downloaded_ships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_ships.to_csv(ship_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating non-ship databits\n",
    "\n",
    "Create the non-ship data by iterating over the ship dataset and for every ship we create a random point inside the geojson & request the same envelope size. We check that no ship is inside the selected region. If this check fails we repeat until we actually have a successfull download. For each order a dataframe entry is created and the corresponding ship is marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_ships['non_ship'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_non_ship(url, index, file_path):\n",
    "    #filepath = 'downloaded_non_ships\\\\AIS_Filename'\n",
    "    data_dir = os.path.join(os.getcwd(), 'data')\n",
    "    url = url\n",
    "    save_path = os.path.join(data_dir, file_path)\n",
    "    idx_save_path = os.path.join(save_path, str(index))\n",
    "    \n",
    "    # create a folder for the downloaded file\n",
    "    mkdir_command =\"mkdir \" + str(idx_save_path)\n",
    "    subprocess.Popen(mkdir_command, stdout=subprocess.PIPE, shell=True, stderr = subprocess.PIPE)\n",
    "\n",
    "    # construct the porder command\n",
    "    command_1 = \"porder download --url \\\"\" + str(url) + \"\\\" \"\n",
    "    command_2 = \"--local \\\"\" + str(idx_save_path) + \"\\\"\"\n",
    "    command = command_1 + command_2 \n",
    "\n",
    "    query = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, stderr = subprocess.PIPE)\n",
    "    result = query.stdout.read().decode(\"utf-8\")\n",
    "    print(result)\n",
    "    #extract the correct basefilename\n",
    "    try:\n",
    "        filename = result[result.find('Downloading:') :].split(' ')[1].split('.')[0]\n",
    "    except IndexError:\n",
    "        try:\n",
    "            filename = result[result.find('SKIPPING:') :].split(' ')[1].split('.')[0]\n",
    "        except IndexError:\n",
    "            filename = 'Error'\n",
    "    return filename\n",
    "\n",
    "def generate_random(number, polygon):\n",
    "    # we generate random points inside a polygon\n",
    "    list_of_points = []\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    counter = 0\n",
    "    while counter < number:\n",
    "        pnt = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "        if polygon.contains(pnt):\n",
    "            list_of_points.append(pnt)\n",
    "            counter += 1\n",
    "    return list_of_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the geojson and the directories\n",
    "geojson_name = 'miami_to_palm.geojson'\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "geojson_dir = os.path.join(data_dir, \"geojson\")#\n",
    "path_to_geojson = os.path.join(geojson_dir, geojson_name)\n",
    "gjson = gpd.read_file(path_to_geojson)\n",
    "gjson = gjson.iloc[0,0]\n",
    "\n",
    "# we create a non_ship_df\n",
    "non_ship_df = pd.DataFrame(columns=['coordinate','envelope','ship_base_name', 'ship_idx','BaseDateTime'])\n",
    "\n",
    "# Iterate over all filenames\n",
    "for index, ship in downloaded_ships.iterrows():\n",
    "    # if that ship already has a non_ship we don't need to create a new one\n",
    "    if ship['non_ship']:\n",
    "        continue\n",
    "    name = '_'.join(ship['base_name'].split('\\\\')[-1].split('_')[:3])\n",
    "    ships = downloaded_ships.loc[downloaded_ships['base_name'].str.contains(name)]\n",
    "    base_name = ship['base_name']\n",
    "    \n",
    "    # we set a counter\n",
    "    non_ship_not_found = True\n",
    "    trials = 0\n",
    "\n",
    "    # we create random points near the original ship, check if they contain ships and if they are downloadable from planet\n",
    "    while non_ship_not_found:\n",
    "        trials += 1\n",
    "        try:\n",
    "            ship_point = shapely.wkt.loads(ship['coordinate'])\n",
    "        except AttributeError:\n",
    "            ship_point = ship['coordinate']\n",
    "        # We have a 0.06 degree region around the original ship\n",
    "        buffer_ship = ship_point.buffer(0.06)\n",
    "        big_ship_envelope = buffer_ship.envelope\n",
    "        while True:\n",
    "            # generate a random point and it's buffer\n",
    "            x = generate_random(1, big_ship_envelope )\n",
    "            point = gpd.GeoDataFrame(geometry = x)\n",
    "            buffer = point.buffer(0.02)\n",
    "            # we check if the generated point is in the geojson\n",
    "            if x[0].within(gjson):\n",
    "                break\n",
    "        non_ship_envelope = buffer.envelope\n",
    "        non_ship_gdf = gpd.GeoDataFrame(geometry = non_ship_envelope)\n",
    "        # extract ship envelopes\n",
    "        try:\n",
    "            ship_envelopes = [shapely.wkt.loads(x) for x in ships['envelope']]\n",
    "        except AttributeError:\n",
    "            ship_envelopes = [x for x in ships['envelope']]\n",
    "        ship_gdf = gpd.GeoDataFrame(geometry = ship_envelopes)\n",
    "        # get intersection - see if there's any ship inside our non ship data\n",
    "        intersection = gpd.overlay(ship_gdf, non_ship_gdf, how='intersection')\n",
    "        if len(intersection) < 1:\n",
    "            # if no intersection\n",
    "            # check if this image is available\n",
    "            non_ship_envelope.to_file(os.path.join(geojson_dir,'nonship.geojson'), driver='GeoJSON')\n",
    "            try:\n",
    "                cost, file = create_id_list('nonship.geojson', datetime.datetime.fromisoformat(ships.iloc[0,:]['datetime']).date())\n",
    "            except TypeError:\n",
    "                cost, file = create_id_list('nonship.geojson', ships.iloc[0,:]['datetime'].date())\n",
    "            # if it's available save it\n",
    "            if cost > 0:\n",
    "                # concatenate if it is\n",
    "                this_non_ship_df = pd.DataFrame({'coordinate':x,'envelope':[non_ship_envelope[0]], 'ship_base_name':base_name})\n",
    "                this_non_ship_df['ship_idx']= index\n",
    "                this_non_ship_df['BaseDateTime']=ship['BaseDateTime']\n",
    "                non_ship_df = pd.concat([non_ship_df, this_non_ship_df])\n",
    "                # stop the loop\n",
    "                trials = 0\n",
    "                non_ship_not_found = False\n",
    "                print('found this one ', base_name)\n",
    "        if trials > 60:\n",
    "            # After 60 trials we skip this ship and\n",
    "            # restart the loop\n",
    "            trials = 0\n",
    "            non_ship_not_found = False\n",
    "            print('skipped this one ', base_name)\n",
    "print(non_ship_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering and downloading the non ships\n",
    "\n",
    "This works just as it did for the ships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_non_ships = pd.DataFrame()\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "geojson_dir = os.path.join(data_dir, 'geojson')\n",
    "geojson = os.path.join(geojson_dir, 'test.geojson')\n",
    "\n",
    "for index, ship in non_ship_df.iterrows():\n",
    "    current_ship = ship.copy()\n",
    "    gpd.GeoSeries([ship['envelope']]).to_file(geojson, driver='GeoJSON')\n",
    "    date = datetime.datetime.fromisoformat(ship['BaseDateTime']).date()\n",
    "    cost, filename = create_id_list(geojson, date)\n",
    "    if cost == 0:\n",
    "        continue\n",
    "    else:\n",
    "        times = get_unique_times(filename)\n",
    "        if len(times) > 1:\n",
    "            # this could be done nicer... like using the extra data\n",
    "            continue\n",
    "\n",
    "        time_dif = datetime.datetime.utcfromtimestamp(times[0].astype('O')/1e9) - datetime.datetime.fromisoformat(ship['BaseDateTime'])\n",
    "        # We only order if the time is correct\n",
    "        if abs(time_dif) < datetime.timedelta(minutes=1):       \n",
    "            # if we catch a ship we request and download the asset.\n",
    "            # we order the asset\n",
    "            url = order_ship(ship, filename)\n",
    "            current_ship['url'] = url\n",
    "            downloaded_non_ships = downloaded_non_ships.append(current_ship)\n",
    "downloaded_non_ships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_non_ships.to_csv(non_ship_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait before running the next cells!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the non ships\n",
    "\n",
    "Again just as for the ships. Take care to wait before running this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_non_ships['base_name'] = np.nan\n",
    "downloaded_non_ships.index = pd.RangeIndex(downloaded_non_ships.shape[0])\n",
    "base_path = 'downloaded_non_ships'\n",
    "base_path = os.path.join(base_path, file_extension)\n",
    "\n",
    "for index, row in downloaded_non_ships.iterrows():\n",
    "    url = row.loc['url']\n",
    "    base_name = download_non_ship(url, index, base_path)\n",
    "    downloaded_non_ships.loc[index, 'base_name'] = base_name\n",
    "    \n",
    "downloaded_non_ships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering missing non ships\n",
    "\n",
    "We probably have a little less non ships than ships, and we don't want that in order to have a balanced dataset. So we mark already downloaded ships and can restart the whole process from creating non-ships.\n",
    "\n",
    "You probably have to run the following cells a couple of times to get as many balanced samples as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in downloaded_non_ships.iterrows():\n",
    "    downloaded_ships.loc[row['ship_idx'], 'non_ship'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the geojson and the directories\n",
    "geojson_name = 'miami_to_palm.geojson'\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "geojson_dir = os.path.join(data_dir, \"geojson\")#\n",
    "path_to_geojson = os.path.join(geojson_dir, geojson_name)\n",
    "gjson = gpd.read_file(path_to_geojson)\n",
    "gjson = gjson.iloc[0,0]\n",
    "left_over_count = 0\n",
    "\n",
    "\n",
    "next_non_ship_df = pd.DataFrame(columns=['coordinate','envelope','ship_base_name', 'ship_idx','BaseDateTime'])\n",
    "\n",
    "# Iterate over all filenames\n",
    "for index, ship in downloaded_ships.iterrows():\n",
    "    if ship['non_ship']:\n",
    "        # we only act if there is no corresponding non ship yet\n",
    "        continue\n",
    "    left_over_count += 1\n",
    "    name = '_'.join(ship['base_name'].split('\\\\')[-1].split('_')[:3])\n",
    "    ships = downloaded_ships.loc[downloaded_ships['base_name'].str.contains(name)]\n",
    "    base_name=downloaded_ships['base_name']\n",
    "    non_ship_not_found = True\n",
    "    trials = 0\n",
    "    #print(ship)\n",
    "\n",
    "    while non_ship_not_found:\n",
    "        trials += 1\n",
    "        base_name=ship['base_name']\n",
    "        try:\n",
    "            ship_point = shapely.wkt.loads(ship['coordinate'])\n",
    "        except AttributeError:\n",
    "            ship_point = ship['coordinate']\n",
    "        buffer_ship = ship_point.buffer(0.06)\n",
    "        big_ship_envelope = buffer_ship.envelope\n",
    "        #print(big_ship_envelope)\n",
    "        while True:\n",
    "            # generate a random point and it's buffer\n",
    "            x = generate_random(1, big_ship_envelope )\n",
    "            point = gpd.GeoDataFrame(geometry = x)\n",
    "            buffer = point.buffer(0.02)\n",
    "            # we check if the generated point is in the geojson\n",
    "            if x[0].within(gjson):\n",
    "                break\n",
    "        non_ship_envelope = buffer.envelope\n",
    "        non_ship_gdf = gpd.GeoDataFrame(geometry = non_ship_envelope)\n",
    "        # extract ship envelopes\n",
    "        try:\n",
    "            ship_envelopes = [shapely.wkt.loads(x) for x in ships['envelope']]\n",
    "        except AttributeError:\n",
    "            ship_envelopes = [x for x in ships['envelope']]\n",
    "        ship_gdf = gpd.GeoDataFrame(geometry = ship_envelopes)\n",
    "        # get intersection\n",
    "        intersection = gpd.overlay(ship_gdf, non_ship_gdf, how='intersection')\n",
    "        if len(intersection) < 1:\n",
    "            # check if this is available\n",
    "            #print('no intersection')\n",
    "            non_ship_envelope.to_file(os.path.join(geojson_dir,'nonship.geojson'), driver='GeoJSON')\n",
    "            #cost = 0\n",
    "            try:\n",
    "                cost, file = create_id_list('nonship.geojson', datetime.datetime.fromisoformat(ships.iloc[0,:]['datetime']).date())\n",
    "            except TypeError:\n",
    "                cost, file = create_id_list('nonship.geojson', ships.iloc[0,:]['datetime'].date())\n",
    "            #print(cost)\n",
    "            if cost > 0:\n",
    "                # concatenate if it is\n",
    "                this_non_ship_df = pd.DataFrame({'coordinate':x,'envelope':[non_ship_envelope[0]], 'ship_base_name':base_name})\n",
    "                this_non_ship_df['ship_idx']= index\n",
    "                this_non_ship_df['BaseDateTime']=ship['BaseDateTime']\n",
    "                next_non_ship_df = pd.concat([next_non_ship_df, this_non_ship_df])\n",
    "                # restart the loop\n",
    "                trials = 0\n",
    "                non_ship_not_found = False\n",
    "                print('found this one ')\n",
    "        #else:\n",
    "            #print('point in intersection')\n",
    "        if trials > 100:\n",
    "            # After 20 trials we skip\n",
    "            # restart the loop\n",
    "            trials = 0\n",
    "            non_ship_not_found = False\n",
    "            print('skipped this one ')\n",
    "print(next_non_ship_df)\n",
    "\n",
    "print(left_over_count)\n",
    "print(next_non_ship_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_downloaded_non_ships = pd.DataFrame()\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "geojson_dir = os.path.join(data_dir, 'geojson')\n",
    "geojson = os.path.join(geojson_dir, 'test.geojson')\n",
    "\n",
    "for index, ship in next_non_ship_df.iterrows():\n",
    "    current_ship = ship.copy()\n",
    "    gpd.GeoSeries([ship['envelope']]).to_file(geojson, driver='GeoJSON')\n",
    "    date = datetime.datetime.fromisoformat(ship['BaseDateTime']).date()\n",
    "    cost, filename = create_id_list(geojson, date)\n",
    "    if cost == 0:\n",
    "        continue\n",
    "    else:\n",
    "        times = get_unique_times(filename)\n",
    "        if len(times) > 1:\n",
    "            # this could be done nicer... like using the extra data\n",
    "            continue\n",
    "\n",
    "        time_dif = datetime.datetime.utcfromtimestamp(times[0].astype('O')/1e9) - datetime.datetime.fromisoformat(ship['BaseDateTime'])\n",
    "        # We only order if the time is correct\n",
    "        if abs(time_dif) < datetime.timedelta(minutes=1):       \n",
    "            # if we catch a ship we request and download the asset.\n",
    "            # we order the asset\n",
    "            url = order_ship(ship, filename)\n",
    "            current_ship['url'] = url\n",
    "            next_downloaded_non_ships = next_downloaded_non_ships.append(current_ship)\n",
    "next_downloaded_non_ships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again! Wait! Check if the order is ready**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_downloaded_non_ships['base_name'] = np.nan\n",
    "all_non_ships = downloaded_non_ships.shape[0] + next_downloaded_non_ships.shape[0]\n",
    "next_downloaded_non_ships.index = pd.RangeIndex(start=downloaded_non_ships.shape[0], stop=all_non_ships)\n",
    "base_path = 'downloaded_non_ships'\n",
    "base_path = os.path.join(base_path, file_extension)\n",
    "\n",
    "\n",
    "for index, row in next_downloaded_non_ships.iterrows():\n",
    "    url = row.loc['url']\n",
    "    base_name = download_non_ship(url, index, base_path)\n",
    "    next_downloaded_non_ships.loc[index, 'base_name'] = base_name\n",
    "    downloaded_ships.loc[row['ship_idx'], 'non_ship'] = True\n",
    "    \n",
    "next_downloaded_non_ships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_non_ships = pd.concat([downloaded_non_ships, next_downloaded_non_ships])\n",
    "downloaded_non_ships.to_csv(non_ship_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the oversampled non-ships\n",
    "\n",
    "Most of the non-ships can be created in the way shown above, but some are just super resistant. To fill our dataset despite those, we oversample some other non ships to get an even dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the geojson and the directories\n",
    "geojson_name = 'miami_to_palm.geojson'\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "geojson_dir = os.path.join(data_dir, \"geojson\")#\n",
    "path_to_geojson = os.path.join(geojson_dir, geojson_name)\n",
    "gjson = gpd.read_file(path_to_geojson)\n",
    "gjson = gjson.iloc[0,0]\n",
    "left_over_count = 0\n",
    "# Add how many you would like to sample\n",
    "num_sample = 6\n",
    "\n",
    "\n",
    "next_non_ship_df = pd.DataFrame(columns=['coordinate','envelope','ship_base_name', 'ship_idx','BaseDateTime'])\n",
    "\n",
    "# Iterate over all filenames\n",
    "for index, ship in downloaded_ships.sample(num_sample, axis=0).iterrows():\n",
    "    #if ship['non_ship']:\n",
    "    #    continue\n",
    "    left_over_count += 1\n",
    "    name = '_'.join(ship['base_name'].split('\\\\')[-1].split('_')[:3])\n",
    "    ships = downloaded_ships.loc[downloaded_ships['base_name'].str.contains(name)]\n",
    "    base_name=downloaded_ships['base_name']\n",
    "    non_ship_not_found = True\n",
    "    trials = 0\n",
    "    #print(ship)\n",
    "\n",
    "    while non_ship_not_found:\n",
    "        trials += 1\n",
    "        base_name=ship['base_name']\n",
    "        try:\n",
    "            ship_point = shapely.wkt.loads(ship['coordinate'])\n",
    "        except AttributeError:\n",
    "            ship_point = ship['coordinate']\n",
    "        buffer_ship = ship_point.buffer(0.06)\n",
    "        big_ship_envelope = buffer_ship.envelope\n",
    "        #print(big_ship_envelope)\n",
    "        while True:\n",
    "            # generate a random point and it's buffer\n",
    "            x = generate_random(1, big_ship_envelope )\n",
    "            point = gpd.GeoDataFrame(geometry = x)\n",
    "            buffer = point.buffer(0.02)\n",
    "            # we check if the generated point is in the geojson\n",
    "            if x[0].within(gjson):\n",
    "                break\n",
    "        non_ship_envelope = buffer.envelope\n",
    "        non_ship_gdf = gpd.GeoDataFrame(geometry = non_ship_envelope)\n",
    "        # extract ship envelopes\n",
    "        try:\n",
    "            ship_envelopes = [shapely.wkt.loads(x) for x in ships['envelope']]\n",
    "        except AttributeError:\n",
    "            ship_envelopes = [x for x in ships['envelope']]\n",
    "        ship_gdf = gpd.GeoDataFrame(geometry = ship_envelopes)\n",
    "        # get intersection\n",
    "        intersection = gpd.overlay(ship_gdf, non_ship_gdf, how='intersection')\n",
    "        if len(intersection) < 1:\n",
    "            # check if this is available\n",
    "            #print('no intersection')\n",
    "            non_ship_envelope.to_file(os.path.join(geojson_dir,'nonship.geojson'), driver='GeoJSON')\n",
    "            #cost = 0\n",
    "            try:\n",
    "                cost, file = create_id_list('nonship.geojson', datetime.datetime.fromisoformat(ships.iloc[0,:]['datetime']).date())\n",
    "            except TypeError:\n",
    "                cost, file = create_id_list('nonship.geojson', ships.iloc[0,:]['datetime'].date())\n",
    "            #print(cost)\n",
    "            if cost > 0:\n",
    "                # concatenate if it is\n",
    "                this_non_ship_df = pd.DataFrame({'coordinate':x,'envelope':[non_ship_envelope[0]], 'ship_base_name':base_name})\n",
    "                this_non_ship_df['ship_idx']= index\n",
    "                this_non_ship_df['BaseDateTime']=ship['BaseDateTime']\n",
    "                next_non_ship_df = pd.concat([next_non_ship_df, this_non_ship_df])\n",
    "                # restart the loop\n",
    "                trials = 0\n",
    "                non_ship_not_found = False\n",
    "                print('found this one ')\n",
    "        #else:\n",
    "            #print('point in intersection')\n",
    "        if trials > 100:\n",
    "            # After 20 trials we skip\n",
    "            # restart the loop\n",
    "            trials = 0\n",
    "            non_ship_not_found = False\n",
    "            print('skipped this one ')\n",
    "print(next_non_ship_df)\n",
    "\n",
    "print(left_over_count)\n",
    "print(next_non_ship_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to download those ships using the download code from above. :)\n",
    "\n",
    "Now we have all the data available for the whole month. We could clean the file system up and combine some things... \n",
    "But it's the backbone of a decent satellite imagery ship dataset :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Known issues:\n",
    "- Some wrong labeled non-ships are downloaded and created. Those were found using visual inspection. It was pretty obvious.\n",
    "- We do have some cloud covered datapoints.\n",
    "- In some ship samples we cannot see the ship - This can be due to ship size, wrong AIS data or the ship moved to far in the past minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
